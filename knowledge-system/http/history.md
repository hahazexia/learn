# http 历史

## 带宽和延迟

* 带宽：但是现在网络基础建设已经使得带宽得到极大的提升，我们不再会担心由带宽而影响网速，那么就只剩下延迟了。
* 延迟
    * 队头阻塞（head of line blocking）一列的第一个数据包（队头）受阻而导致整列数据包受阻。
    * DNS 查询 将域名解析为 IP 的这个系统就是 DNS。这个通常可以利用DNS缓存结果来达到减少这个时间的目的。
    * 建立连接 HTTP 是基于 TCP 协议的，浏览器最快也要在第三次握手时才能捎带 HTTP 请求报文，达到真正的建立连接，但是这些连接无法复用会导致每次请求都经历三次握手和慢启动。

## http 0.9 http 1.0 和 http 1.1

### http 0.9
HTTP/0.9 只支持一种方法 GET，请求只有一行。

```js
GET /hello.html
```

响应也是非常简单的，只包含 html 文档本身。

```html
<html>
Hello world
</html>
```

当 TCP 建立连接之后，服务器向客户端返回 html 格式的字符串。发送完毕后，就关闭 TCP 连接。由于没有状态码和错误代码，如果服务器处理的时候发生错误，只会传回一个特殊的包含问题描述信息的 html 文件。这就是最早的 HTTP/0.9 版本。

### http 1.0

相比 HTTP/0.9，HTTP/1.0 主要有如下特性：

* 请求与响应支持 HTTP 头，增加了状态码，响应对象的一开始是一个响应状态行
* 协议版本信息需要随着请求一起发送，支持 HEAD，POST 方法
* 支持传输 html 文件以外其他类型的内容

一个典型的 HTTP/1.0 的请求像这样：

```js
GET /hello.html HTTP/1.0
User-Agent:NCSA_Mosaic/2.0(Windows3.1)

200 OK
Date: Tue, 15 Nov 1996 08:12:31 GMT
Server: CERN/3.0 libwww/2.17
Content-Type: text/html

<html>
页面
</html>

```

### http 1.1

在 HTTP/1.0 发布几个月后，HTTP/1.1 就发布了。HTTP/1.1 更多的是作为对 HTTP/1.0 的完善，在 HTTP/1.1 中，主要具有如下改进：

* 默认启用长连接，connection: keep-alive。也就是三次握手后不会立即断开 TCP 连接，可以继续在连接上发后续的请求。
* 增加 pipelining。如果没有管道（pipelining），浏览器必须等待发送第二个资源请求，直到第一个请求的响应被完全接收（使用 Content-Length）。这会为每个请求增加一个往返时间（RTT）延迟，这对Web性能不利。有了管道，浏览器不必等待第一个请求响应数据，现在可以背靠背地发送请求。`服务器必须按照接收请求的顺序发送对这些 pipelining 请求的响应`。换一种说法：`管道解决了请求的队头阻塞，而不是响应的队头阻塞`。
* 引入更多缓存机制 Cache-control Last-Modified & If-Modified-Since ETag & If-None-Match
* 新增了 OPTIONS PUT DELETE TRACE CONNECT 方法

缺点：

* 传输内容是明文，不够安全
* header 内容过大，每次请求 header 变化不大，造成浪费
* keep-alive 给服务端带来性能压力
* 还是没有解决队头阻塞问题

![pipeline](../img/pipelining.png)


### https

https 其实就是将 http 的数据包再通过 SSL/TLS 加密后传输，为了解决 http 1.1 明文传输不安全的问题。

SSL（Secure Sockets Layer）安全套接层和 TLS（Transport Layer Security）传输层安全协议其实是一套东西。

网景公司在1994年提出 HTTPS 协议时，使用的是 SSL 进行加密。后来 IETF（Internet Engineering Task Force）互联网工程任务组将 SSL 进一步标准化，于 1999 年公布第一版 TLS 协议文件 TLS 1.0。目前最新版的 TLS 协议是 TLS 1.3，于 2018 年公布。

https 加密解密流程：

1. 用户在浏览器发起 HTTPS 请求，默认使用服务端的 443 端口进行连接；
2. HTTPS 需要使用一套 CA 数字证书，证书内会附带一个公钥 Public，而与之对应的私钥 Private 保留在服务端不公开；
3. 服务端收到请求，返回配置好的包含公钥 Public 的证书给客户端；
4. 客户端收到证书，校验合法性，主要包括是否在有效期内、证书的域名与请求的域名是否匹配，上一级证书是否有效（递归判断，直到判断到系统内置或浏览器配置好的根证书），如果不通过，则显示 HTTPS 警告信息，如果通过则继续；
5. 客户端生成一个用于对称加密的随机 key，并用证书内的公钥 Public 进行加密，发送给服务端；
6. 服务端收到随机 key 的密文，使用与公钥 Public 配对的私钥 Private 进行解密，得到客户端真正想发送的随机 key；
7. 服务端使用客户端发送过来的随机 key 对要传输的数据进行对称加密，将密文返回客户端；
8. 客户端使用随机 key 对称解密密文，得到数据明文；
9. 后续 HTTPS 请求使用之前交换好的随机 Key 进行对称加解密。

简化一下：
1. 客户端请求服务端，服务端将 CA 证书返回给客户端
2. 客户端验证 CA 证书 的合法性，如果不合法就警告，如果合法，就生成随机 key，并且用 CA 证书中的 public 公钥加密，然后发给服务端
3. 服务端收到加密 key，用 private 私钥解密，得到 key
4. 服务端将响应数据用 key 对称加密后返回客户端
5. 客户端用 key 对称解密响应，得到数据明文。

其实就是依赖 CA 证书的公钥私钥互相交换加密 key 的过程。

#### 非对称加密和对称加密

* 对称加密就是双方使用同一个密钥加密和解密。
* 非对称加密就是有两个不同的密钥，公钥和私钥。公钥用来加密，这时密文只能用私钥才能解开。

#### 中间人攻击

* 当服务端向客户端返回公钥 A1 的时候，中间人将其替换成自己的公钥 B1 传送给浏览器。
* 而浏览器此时一无所知，傻乎乎地使用公钥 B1 加密了密钥 K 发送出去，又被中间人截获，中间人利用自己的私钥 B2 解密，得到密钥 K，再使用服务端的公钥 A1 加密传送给服务端，完成了通信链路，而服务端和客户端毫无感知。

#### CA 机构

* 客户端无法确认收到的公钥是不是真的是服务端发来的。为了解决这个问题，互联网引入了一个公信机构，这就是 CA。
* 私钥除了解密外的真正用途其实还有一个，就是数字签名，其实就是一种防伪技术，只要有人篡改了证书，那么数字签名必然校验失败。具体过程如下
    1. CA 机构拥有自己的一对公钥和私钥， CA 机构在颁发证书时对证书明文信息进行哈希，将哈希值用私钥进行加签，得到数字签名。明文数据和数字签名组成证书，传递给客户端。
    2. 客户端得到证书，分解成明文部分 Text 和数字签名 Sig1。用 CA 机构的公钥进行解签，得到 Sig2（由于CA机构是一种公信身份，因此在系统或浏览器中会内置 CA 机构的证书和公钥信息）
    3. 用证书里声明的哈希算法对明文 Text 部分进行哈希得到 T。当自己计算得到的哈希值 H 与解签后的 Sig2 相等，表示证书可信，没有被篡改

非对称加密的签名过程是，私钥将一段消息进行加签，然后将签名部分和消息本身一起发送给对方，收到消息后对签名部分利用公钥验签，如果验签出来的内容和消息本身一致，表明消息没有被篡改。


在这个过程中，系统或浏览器中内置的 CA 机构的证书和公钥成为了至关重要的环节，这也是 CA 机构公信身份的证明，如果系统或浏览器中没有这个 CA 机构，那么客户端可以不接受服务端传回的证书，显示 HTTPS 警告。

实际上 CA 机构的证书是一条信任链，A 信任 B，B 信任 C，以掘金的证书为例，掘金向 RapidSSL 申请一张证书，而 RapidSSL 的 CA 身份是由 DigiCert Global 根 CA 认证的，构成了一条信任链。

各级 CA 机构的私钥是绝对的私密信息，一旦 CA 机构的私钥泄露，其公信力就会一败涂地。之前就有过几次 CA 机构私钥泄露，引发信任危机，各大系统和浏览器只能纷纷吊销内置的对应 CA 的根证书。

有些老旧的网站会要求使用前下载安装他自己的根证书，这就是这个网站使用的证书并不能在系统内置的 CA 机构和根证书之间形成一条信任链，需要自己安装根证书来构成信任链，这里的风险就要使用者自己承担了。


### SPDY

2012 年 Google 一声惊雷提出了 SPDY (发音为"speedy") 的方案，主要解决以下问题：

1. 降低延迟：SPDY 采取了多路复用（Multiplexing）。多路复用通过多个请求 stream 共享一个 TCP 连接的方式，降低了创建多个 TCP 的延迟同时提高了带宽的利用率。
2. 请求优先级（Request Prioritization）： 多路复用带来一个新的问题是，在连接共享的基础之上有可能会导致关键请求被阻塞。SPDY 允许给每个 request 设置优先级，这样重要的请求就会优先得到响应。比如浏览器加载首页，首页的 html 内容应该优先展示，之后才是各种静态资源文件，脚本文件等加载，这样可以保证用户能第一时间看到网页内容。
3. header压缩： 前面提到 HTTP1.x 的 header 很多时候都是重复多余的，而有些 header 的内容在不压缩的情况下则比较“庞大”（例如 cookie 和user-agent 等）。选择合适的压缩算法可以减小包的大小和数量，不仅可以节省资源，还可以缩短数据传递的延迟。
4. 基于 HTTPS 的加密协议传输： 保留了 HTTPS 的 TLS 加密特性，大大提高了传输数据的可靠性。
5. 服务端推送（Server Push）： 采用了 SPDY 的网页，例如我的网页有一个 sytle.css 的请求，在客户端收到 sytle.css 数据的同时，服务端会将 index.js 的文件推送给客户端，当客户端再次尝试获取 index.js 时就可以直接从缓存中获取到，不用再发请求了。

### http 2.0

http 2.0 是基于 SPDY 的。区别：

1. HTTP2.0消息头的压缩算法采用HPACK算法，而非SPDY采用的DEFLATE算法。
2. HTTP2.0设计初期支持明文HTTP传输，而SPDY强制使用HTTPS，到后期两者都需要使用HTTPS。

新特性：

1. 新的二进制格式（Binary Format）： HTTP1.x 的解析是基于文本。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认 0 和 1 的组合。基于这种考虑 HTTP2.0 的协议解析决定采用二进制格式，实现方便且健壮。
2. 多路复用（MultiPlexing）： 即连接共享，即每一个 request 都是是用作连接共享机制的。一个 request 对应一个 id，这样一个连接上可以有多个request，每个连接的 request 可以随机的混杂在一起，接收方可以根据 request 的 id 将 request 再归属到各自不同的服务端请求里面。
3. header 压缩： 如上文中所言，对前面提到过 HTTP1.x 的 header 带有大量信息，而且每次都要重复发送，HTTP2.0 使用 encoder 来减少需要传输的header 大小，通讯双方各自 cache 一份 header fields 表，既避免了重复 header 的传输，又减小了需要传输的大小。
4. 服务端推送（Server Push）： 同 SPDY 一样，HTTP2.0 也具有 Server Push 功能。

#### http 2.0 和 http 1.1 的区别

##### 帧和流

* 在 HTTP/2 中，有两个非常重要的概念：帧（frame）和流（stream）
* 在 HTTP 2.0 中，它把数据报的两大部分分成了 header frame 和 data frame。也就是头部帧和数据体帧。帧的传输最终在流中进行，流中的帧，头部(header)帧 和 data 帧可以分为多个片段帧，例如 data 帧即是可以 data = data_1 + data_2 + ... + data_n。
* 流代表了一个完整的请求-响应数据交互过程。它具有如下几个特点：
    + 双向性：同一个流内，可同时发送和接受数据。
    + 有序性：流中被传输的数据就是二进制帧 。帧在流上的被发送与被接收都是按照顺序进行的。
    + 并行性：流中的 二进制帧 都是被并行传输的，无需按顺序等待。但却不会引起数据混乱，因为每个帧都有顺序标号。它们最终会被按照顺序标号来合并。
    + 流的创建：流可以被客户端或服务器单方面建立, 使用或共享。
    + 流的关闭：流也可以被任意一方关闭。

##### 发展历程

1. 多个 Tcp 连接（http 1.0）

在最早的时候没有keep-alive只能创建多个Tcp连接来做多次请求。一次请求完成就会关闭本次的 Tcp 连接，下个请求又要从新建立 Tcp 连接传输完成数据再关闭，造成很大的性能损耗。

2. Keep-Alive (http 1.1)

一定时间内，同一域名多次请求数据，只建立一次 HTTP 请求，其他请求可复用每一次建立的连接通道。这样就不用反复建立和断开连接了。

3. pipeline (http 1.1)

如果没有管道（pipelining），浏览器必须等待发送第二个资源请求，直到第一个请求的响应被完全接收（使用 Content-Length）。这会为每个请求增加一个往返时间（RTT）延迟，这对Web性能不利。有了管道，浏览器不必等待第一个请求响应数据，现在可以背靠背地发送请求。`服务器必须按照接收请求的顺序发送对这些 pipelining 请求的响应`。换一种说法：`管道解决了请求的队头阻塞，而不是响应的队头阻塞`。

4. 多路复用（http 2.0）

在一个TCP连接中可以存在多条流，也就是可以发送多个请求，对端可以通过帧中的标识知道属于哪个请求，并且数据被拆成多个帧，拥有序号，等到所有数据帧都被接收后再按照顺序组合成完整的数据。通过这个技术，可以避免旧版本中的队头阻塞问题，极大的提高传输性能。总结一句话就是单个连接上可以并行交错的请求和响应，之间互不干扰。

##### 区别

* http 1.1 是基于文本传输的，http 2.0 使用了二进制来传输数据，实现了帧和流
* http 1.1 使用了 keep-alive 和 pipeline，仍然没有解决响应的队头阻塞问题，而 http 2.0 采用了多路复用，一个连接中可以有多条流，多个请求和响应互不干扰，解决了队头阻塞问题。
* http 2.0 的 header 做了压缩，而且客户端和服务器各自缓存了 header fields 表，重复 header 不再传输
* 服务端推送，还没有收到浏览器的后续的请求，服务器就主动把各种后续需要的资源推送给浏览器。比如，浏览器只请求了index.html，但是服务器把index.html、style.css、example.png全部发送给浏览器。

##### http 2.0 的缺陷

HTTP/2 是通过分帧并且给每个帧打上流的 ID 去避免依次响应的问题，对方接收到帧之后根据 ID 拼接出流，这样就可以做到乱序响应从而避免请求时的队首阻塞问题。但是 TCP 层面的队首阻塞是 HTTP/2 无法解决的（HTTP 只是应用层协议，TCP 是传输层协议），TCP 的阻塞问题是因为传输阶段可能会丢包，一旦丢包就会等待重新发包，阻塞后续传输。

### http 3.0 和 QUIC 协议（quick）

传统的 HTTP 协议是基于传输层 TCP 的协议，而 QUIC 是基于传输层 UDP 上的协议，可以定义成：HTTP3.0 是基于 UDP 的安全可靠的 HTTP2.0 协议。

它解决了什么问题：

1. 减少了 TCP 三次握手及 TLS 握手时间
2. 多路复用丢包时的线头阻塞问题： QUIC 保留了 HTTP2.0 多路复用的特性，但是即使在多路复用过程中，同一个 TCP 连接上有多个 stream，假如其中一个stream 丢包，在重传前后续的 stream 都会受到影响，而 QUIC 中一个连接上的多个 stream 之间没有依赖。所以当发生丢包时，只会影响当前的 stream，也就避免了线头阻塞问题。
3. 优化重传策略： 以往的 TCP 丢包重传策略是：在发送端为每一个封包标记一个编号 (sequence number)，接收端在收到封包时，就会回传一个带有对应编号的 ACK 封包给发送端，告知发送端封包已经确实收到。当发送端在超过一定时间之后还没有收到回传的 ACK，就会认为封包已经丢失，启动重新传送的机制，复用与原来相同的编号重新发送一次封包，确保在接收端这边没有任何封包漏接。

这样的机制就会带来一些问题，假设发送端总共对同一个封包发送了两次 (初始 + 重传)，使用的都是同一个sequence number：编号 N。之后发送端在拿到编号N 封包的回传 ACK 时，将无法判断这个带有编号 N 的 ACK，是接收端在收到初始封包后回传的 ACK。这就会加大后续的重传计算的耗时。QUIC为了避免这个问题，发送端在传送封包时，初始与重传的每一个封包都改用一个新的编号，unique packet number，每一个编号都唯一而且严格递增，这样每次在收到 ACK 时，就可以依据编号明确的判断这个ACK是来自初始封包或者是重传封包。


### 队头阻塞 Head-of-Line blocking

* http 1.1 的队头阻塞

如果浏览器请求了文件 script.js 和 style.css 文件，会使用 Content-Length 来知道每个响应的结束位置和另一个响应的开始位置。假设 JS 文件比 CSS 大得多。这种情况下，在下载整个 JS 文件之前，CSS 必须等待，尽管它要小得多，其实可以更早地解析。这很容易解决，只需让浏览器在 JS 文件之前请求 CSS 文件。然而，至关重要的是，浏览器无法预先知道这两个文件中的哪一个在请求时会成为更大的文件。

这是 http 1.1 协议设计方式的一个限制。如果只有一个 http 1.1 连接，那么在切换到发送新资源之前，必须完整地传输资源响应。如果前面的资源创建缓慢（例如，从数据库查询动态生成的index.html）或者，如上所述，如果前面的资源很大。这些问题可能会引起队头阻塞问题。这就是为什么浏览器开始为 http 1.1  上的每个页面加载打开多个并行 TCP 连接（通常为6个）。这样，请求可以分布在这些单独的连接上，并且不再有队头阻塞。

这是可行的，但有相当大的开销：建立一个新的 TCP 连接可能是昂贵的（例如在服务器上的状态和内存方面，以及设置 TLS 加密的计算），并且需要消耗一些时间（特别是对于 HTTPS 连接，因为 TLS 需要自己的握手）。

总结：一个大或慢的响应会阻塞之后的其他响应，因此 http 1.1 时期浏览器会同时开启多个 TCP 连接解决此问题，但是多个 TCP 连接会占用更多的内存，而且也增加了握手时间。

* http 2.0 的队头阻塞

http 2.0 为了解决 http 1.1 的这个问题，将 TCP 数据包传递的数据定义成数据帧（data frame）和头部帧（headers frame）这种格式，并且每个帧都标识了一个流 id （stream id）。也就是说一个请求对应一个流（stream），通过流 id 就可以知道这个数据帧属于哪个流（请求），因此就可以在 TCP 连接中交错地发送不同流（请求）的数据帧，等到接收到以后再按照组合成完整的响应数据。

总结：http 2.0 在单个 TCP 连接上通过交错排列块来多路传输多个资源，而不必等待大或慢的响应，可以在同时接收其他资源的数据帧，解决了 http 1.1 的队头阻塞。

* TCP 队头阻塞

TCP 只是传输层协议，它在传输数据包的时候，并不知道这个数据是什么，它只知道它被赋予了一系列字节，它必须从一台计算机传输另一台计算机。它使用特定最大大小（maximum size）的数据包，通常大约为 1450 字节。每个数据包只跟踪它携带的数据的那一部分（字节范围），这样原始数据就可以按照正确的顺序重建。

互联网是一个根本不可靠的网络。在从一个端点到另一个端点的传输过程中，数据包会丢失和延迟。

例子：如果正在传输三个数据包，数据包1（streamid 1），数据包2（streamid 2），数据包3（streamid 1 & streamid 2），并且他们包含两个文件，如果 TCP 数据包2在网络中丢失，但数据包1和数据包3已经到达，会发生什么情况？因此，它知道数据包1的内容可以安全使用，并将这些内容传递给浏览器。然而，它发现数据包1中的字节和数据包3中的字节（放数据包2 的地方）之间存在间隙，因此还不能将数据包3传递给浏览器。TCP 将数据包3保存在其接收缓冲区（receive buffer）中，直到它接收到数据包2的重传副本（这至少需要往返服务器一次），之后它可以按照正确的顺序将这两个数据包都传递给浏览器。换个说法：丢失的数据包2 队头阻塞（HOL blocking）数据包3！

总结：如果一个 TCP 包丢失，所有后续的包都需要等待它的重传，即使它们包含来自不同流的无关联数据。TCP 具有传输层队头阻塞。

* http 3.0 QUIC

TCP 本身不知道什么是流（stream）和帧（frame），因此需要一个新的协议，于是基于 UDP 协议，出现了 QUIC 协议。QUIC 协议将数据帧中的 streamid 放到了 QUIC 数据包中，于是传输过程中就有了流的概念，也就是说将 HTTP 层的流转移到了 QUIC 中（将应用层中的流转移到了传输层中）。

